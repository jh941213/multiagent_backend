# stock_report_pipeline.py

import ast
import operator
from typing import List, Annotated
from typing_extensions import TypedDict
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_teddynote.graphs import visualize_graph  # ì‹œê°í™” í•¨ìˆ˜(ì‚¬ìš© ì•ˆ í•  ê²½ìš° ì œê±° ê°€ëŠ¥)
from langgraph.graph import START, END, StateGraph
from langgraph.constants import Send
from langgraph.checkpoint.memory import MemorySaver

# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬
# from IPython.display import Markdown  # ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì „ìš©
# import IPython.display  # ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì „ìš©

# LLM ì—°ê²°
from langchain_aws import ChatBedrock

llm = ChatBedrock(
    model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",
    model_kwargs=dict(temperature=0, max_tokens=8192),
    credentials_profile_name="default",
    region_name="us-east-1"
)


##############################
# 1) KDBanalyst í´ë˜ìŠ¤ ì •ì˜
##############################
class KDBanalyst(BaseModel):
    affiliation: str = Field(
        description="ë¦¬í¬í„°ì˜ ì†Œì† íšŒì‚¬",
        default=""
    )
    name: str = Field(
        description="ë¦¬í¬í„°ì˜ ì´ë¦„",
        default="ê¹€ì¬í˜„"
    )
    expertise: str = Field(
        description="ì£¼ì‹ ì‹œì¥ ì „ë¬¸ ë¶„ì•¼",
        default="ë‚˜ìŠ¤ë‹¥ ê°œì¸íˆ¬ìì"
    )
    analysis_focus: str = Field(
        description="ì£¼ì‹ ë¶„ì„ì˜ ì£¼ìš” ê´€ì ",
        default="ê¸°ìˆ ì  ë¶„ì„, í€ë”ë©˜í„¸ ë¶„ì„, ì‹œì¥ ë™í–¥ ë¶„ì„"
    )
    investment_style: str = Field(
        description="íˆ¬ì ìŠ¤íƒ€ì¼ê³¼ ì „ëµ",
        default="ë‹¨ê¸° íˆ¬ì ë° ìŠ¤ì¼ˆí•‘ ìœ ì €"
    )
    risk_management: str = Field(
        description="ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë°©ì‹",
        default="í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì‚°, ì†ì ˆë§¤ ê·œì¹™ ì¤€ìˆ˜"
    )

    @property
    def profile(self) -> str:
        return f"""
        ğŸ“Š Stock Reporter Profile ğŸ“Š
        ì†Œì†: {self.affiliation}
        ì´ë¦„: {self.name}
        ì „ë¬¸ë¶„ì•¼: {self.expertise}
        ë¶„ì„ê´€ì : {self.analysis_focus}
        íˆ¬ììŠ¤íƒ€ì¼: {self.investment_style}
        ë¦¬ìŠ¤í¬ê´€ë¦¬: {self.risk_management}
        """

    @property
    def description(self) -> str:
        return f"""
        ì „ë¬¸ë¶„ì•¼: {self.expertise}
        ë¶„ì„ê´€ì : {self.analysis_focus}
        íˆ¬ììŠ¤íƒ€ì¼: {self.investment_style}
        ë¦¬ìŠ¤í¬ê´€ë¦¬: {self.risk_management}
        """


##############################
# 2) Perspectives í´ë˜ìŠ¤ ì •ì˜
##############################
class Perspectives(BaseModel):
    reporters: List[KDBanalyst] = Field(
        description="ë¦¬í¬íŠ¸ ì‘ì„±ì— ì°¸ì—¬í•œ ë¦¬í¬í„° ëª©ë¡",
    )


##############################
# 3) GenerateAnalystsState íƒ€ì… ì •ì˜
##############################
class GenerateAnalystsState(TypedDict):
    topic: str
    max_analysts: int
    human_analyst_feedback: str
    analysts: List[KDBanalyst]


##############################
# 4) ë¶„ì„ê°€ ìƒì„± ê´€ë ¨ í•¨ìˆ˜ ë° í”„ë¡¬í”„íŠ¸
##############################
analyst_instructions = """ì£¼ì‹ ë¶„ì„ê°€ í˜ë¥´ì†Œë‚˜ ìƒì„±ì„ ë‹´ë‹¹í•˜ê²Œ ë©ë‹ˆë‹¤.

ë‹¤ìŒ ì§€ì¹¨ì„ ì£¼ì˜ ê¹Šê²Œ ë”°ë¼ì£¼ì„¸ìš”:
1. ë¨¼ì € ì—°êµ¬ ì£¼ì œë¥¼ ê²€í† í•˜ì„¸ìš”:

{topic}

2. ë¶„ì„ê°€ ìƒì„±ì„ ì•ˆë‚´í•˜ê¸° ìœ„í•´ ì œê³µëœ í¸ì§‘ í”¼ë“œë°±ì„ ê²€í† í•˜ì„¸ìš”:

{human_analyst_feedback}

3. ìœ„ì˜ ë¬¸ì„œ ë°/ë˜ëŠ” í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ì‹ ë¶„ì„ì— í•„ìš”í•œ ì£¼ìš” ê´€ì ì„ íŒŒì•…í•˜ì„¸ìš”.

4. ìƒìœ„ {max_analysts}ê°œì˜ ê´€ì ì„ ì„ íƒí•˜ì„¸ìš”.

5. ê° ê´€ì ë³„ë¡œ ë‹¤ìŒ ì†ì„±ì„ ê°€ì§„ ë¶„ì„ê°€ë¥¼ ìƒì„±í•˜ì„¸ìš”:
   - ì†Œì†: 4ëŒ€ì€í–‰(êµ­ë¯¼, ì‹ í•œ, ìš°ë¦¬, í•˜ë‚˜) ë³„ íˆ¬ìì¦ê¶ŒíšŒì‚¬ ë° ê¸ˆìœµê¸°ê´€
   - ì´ë¦„: í•œêµ­ì‹ ì´ë¦„
   - ì „ë¬¸ë¶„ì•¼: ì£¼ì‹ì‹œì¥ íŠ¹ì • ë¶„ì•¼
   - ë¶„ì„ê´€ì : ê¸°ìˆ ì /í€ë”ë©˜í„¸/ì‹œì¥ë™í–¥ ë¶„ì„ ì¤‘ ì„ íƒ
   - íˆ¬ììŠ¤íƒ€ì¼: ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸° íˆ¬ì ì „ëµ
   - ë¦¬ìŠ¤í¬ê´€ë¦¬: êµ¬ì²´ì ì¸ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë°©ì‹"""

def create_analysts(state: GenerateAnalystsState):
    """ë¶„ì„ê°€ í˜ë¥´ì†Œë‚˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    topic = state["topic"]
    max_analysts = state["max_analysts"]
    human_analyst_feedback = state.get("human_analyst_feedback", "")

    structured_llm = llm.with_structured_output(Perspectives)

    system_message = analyst_instructions.format(
        topic=topic,
        human_analyst_feedback=human_analyst_feedback,
        max_analysts=max_analysts,
    )

    # LLM í˜¸ì¶œ
    analysts = structured_llm.invoke(
        [SystemMessage(content=system_message)]
        + [HumanMessage(content="KDBanalyst í´ë˜ìŠ¤ í˜•ì‹ì— ë§ëŠ” ë¶„ì„ê°€ ì§‘í•©ì„ ìƒì„±í•˜ì„¸ìš”.")]
    )
    return {"analysts": analysts.reporters}


def human_feedback(state: GenerateAnalystsState):
    """ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°›ê¸° ìœ„í•œ ì¤‘ë‹¨ì  ë…¸ë“œ"""
    pass


def should_continue(state: GenerateAnalystsState):
    """ì›Œí¬í”Œë¡œìš°ì˜ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” í•¨ìˆ˜"""
    human_analyst_feedback = state.get("human_analyst_feedback", None)
    if human_analyst_feedback:
        return "create_analysts"
    return END


##############################
# 5) ë¶„ì„ê°€ ê·¸ë˜í”„ ì‘ì„±
##############################
from langgraph.graph import START, END, StateGraph
from langgraph.checkpoint.memory import MemorySaver

builder = StateGraph(GenerateAnalystsState)
builder.add_node("create_analysts", create_analysts)
builder.add_node("human_feedback", human_feedback)

builder.add_edge(START, "create_analysts")
builder.add_edge("create_analysts", "human_feedback")

builder.add_conditional_edges(
    "human_feedback", should_continue, ["create_analysts", END]
)

memory = MemorySaver()
graph = builder.compile(interrupt_before=["human_feedback"], checkpointer=memory)

# í•„ìš” ì‹œ ê·¸ë˜í”„ ì‹œê°í™”
# visualize_graph(graph)


###################################
# 6) ì¸í„°ë·° íë¦„(ì§ˆë¬¸/ê²€ìƒ‰/ë‹µë³€) ê´€ë ¨ ì½”ë“œ
###################################
from langchain_core.messages import get_buffer_string

# ìƒíƒœ ì •ì˜
from langgraph.graph import MessagesState

class InterviewState(MessagesState):
    max_num_turns: int
    context: Annotated[list, operator.add]
    analyst: KDBanalyst
    interview: str
    sections: list


class SearchQuery(BaseModel):
    search_query: str = Field(None, description="Search query for retrieval.")


question_instructions = """ë‹¹ì‹ ì€ Human-in-the-loop ë°©ì‹ìœ¼ë¡œ ì „ë¬¸ê°€ì™€ í˜‘ì—…í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì „ë¬¸ê°€ì™€ì˜ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ë” ë‚˜ì€ ê²°ê³¼ë¬¼ì„ ë„ì¶œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

1. í˜‘ì—…: ì „ë¬¸ê°€ì˜ ë„ë©”ì¸ ì§€ì‹ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ í•¨ê»˜ ë¬¸ì œë¥¼ í•´ê²°
2. ë°˜ë³µ: ì „ë¬¸ê°€ì˜ í”¼ë“œë°±ì„ ë°›ì•„ ì§€ì†ì ìœ¼ë¡œ ê°œì„ 
3. êµ¬ì²´í™”: ì¶”ìƒì ì¸ ì•„ì´ë””ì–´ë¥¼ êµ¬ì²´ì ì¸ í•´ê²°ì±…ìœ¼ë¡œ ë°œì „

ë‹¤ìŒì€ ë‹¹ì‹ ì´ ì§‘ì¤‘í•´ì•¼ í•  ì£¼ì œì™€ ëª©í‘œì…ë‹ˆë‹¤: {goals}

ë¨¼ì € ë‹¹ì‹ ì˜ í˜ë¥´ì†Œë‚˜ì— ë§ëŠ” ì´ë¦„ìœ¼ë¡œ ìì‹ ì„ ì†Œê°œí•˜ê³ , í˜‘ì—…ì˜ ëª©ì ì„ ì„¤ëª…í•˜ì„¸ìš”.

ì „ë¬¸ê°€ì˜ ì˜ê²¬ì„ ê²½ì²­í•˜ê³  ì´í•´í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ì´ì–´ê°€ì„¸ìš”.

ì¶©ë¶„í•œ í˜‘ì—…ì´ ì´ë£¨ì–´ì¡Œë‹¤ê³  íŒë‹¨ë˜ë©´ "í˜‘ì—…í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!"ë¼ê³  ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.

ì „ì²´ ê³¼ì •ì—ì„œ Human-in-the-loop ì² í•™ì— ë§ëŠ” í˜‘ì—…ì ì´ê³  ë°˜ë³µì ì¸ ì ‘ê·¼ì„ ìœ ì§€í•˜ì„¸ìš”."""


def generate_question(state: InterviewState):
    analyst = state["analyst"]
    messages = state["messages"]

    system_msg = question_instructions.format(goals=analyst.profile)
    question = llm.invoke([SystemMessage(content=system_msg)] + messages)

    return {"messages": [question]}


# TavilySearch, YouTubeSearchTool ë“±
from langchain_teddynote.tools.tavily import TavilySearch
from langchain_community.tools import YouTubeSearchTool
from youtube_transcript_api import YouTubeTranscriptApi


def search_web(state: InterviewState):
    tavily_search = TavilySearch(max_results=3)
    structured_llm = llm.with_structured_output(SearchQuery)

    # ë©”ì‹œì§€ë¥¼ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜
    messages = state["messages"]
    search_query = structured_llm.invoke([
        SystemMessage(content="\n\nHuman: ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\n\nAssistant:"),
        HumanMessage(content=f"\n\nHuman: {messages}\n\nAssistant:")
    ])

    # ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰
    search_docs = tavily_search.invoke(search_query.search_query)

    formatted_search_docs = "\n\n---\n\n".join(
        [
            f'<Document href="{doc["url"]}"/>\n{doc["content"]}\n</Document>'
            for doc in search_docs
        ]
    )
    return {"context": [formatted_search_docs]}


def search_youtube(state: InterviewState):
    structured_llm = llm.with_structured_output(SearchQuery)
    messages = state["messages"]

    # ë§ˆì§€ë§‰ AIMessageëŠ” ê²€ìƒ‰ì— ì§ì ‘ ë°˜ì˜í•˜ì§€ ì•Šë„ë¡ í•„í„°ë§(ì˜ˆ: ë¶ˆí•„ìš”í•œ ë°˜ë³µ ë°©ì§€)
    filtered_messages = []
    for msg in messages:
        if isinstance(msg, AIMessage) and msg == messages[-1]:
            continue
        filtered_messages.append(msg)

    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    search_query = structured_llm.invoke([
        SystemMessage(content="\n\nHuman: ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.\n\nAssistant: ë„¤, ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤."),
        HumanMessage(content=f"\n\nHuman: {filtered_messages}\n\nAssistant:")
    ])

    def get_video_info(url):
        try:
            video_id = url.split('watch?v=')[1].split('&')[0]
            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko', 'en'])
            text = "\n".join([item['text'] for item in transcript])
            return {
                'video_id': video_id,
                'url': url,
                'transcript': text
            }
        except Exception as e:
            print(f"ë™ì˜ìƒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None

    try:
        youtube_search_tool = YouTubeSearchTool()
        youtube_results = youtube_search_tool.run(search_query.search_query)
        if isinstance(youtube_results, str):
            youtube_results = ast.literal_eval(youtube_results)

        formatted_results = []
        for url in youtube_results:
            if isinstance(url, str) and 'youtube.com' in url:
                video_info = get_video_info(url)
                if video_info:
                    formatted_results.append(
                        f'<Document source="youtube" url="{video_info["url"]}">\n'
                        f'<Content>\n{video_info["transcript"][:1000]}...\n</Content>\n'
                        f'</Document>'
                    )

        formatted_search_docs = "\n\n---\n\n".join(formatted_results)
        return {"context": [formatted_search_docs]}

    except Exception as e:
        print(f"YouTube ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {
            "context": ["<Error>YouTube ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.</Error>"]
        }


answer_instructions = """ë‹¹ì‹ ì€ ë¶„ì„ê°€ì™€ ì¸í„°ë·°ë¥¼ í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë¶„ì„ê°€ì˜ ê´€ì‹¬ ë¶„ì•¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {goals}

ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì¸í„°ë·°ì–´ê°€ ì œê¸°í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:

{context}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:

1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ì»¨í…ìŠ¤íŠ¸ì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ì™¸ë¶€ ì •ë³´ë‚˜ ê°€ì •ì„ ë„ì…í•˜ì§€ ë§ˆì„¸ìš”.
3. ì»¨í…ìŠ¤íŠ¸ì—ëŠ” ê° ê°œë³„ ë¬¸ì„œì˜ ìƒë‹¨ì— ì¶œì²˜ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
4. ë‹µë³€ì—ì„œ ê´€ë ¨ ì§„ìˆ  ì˜†ì— ì´ëŸ¬í•œ ì¶œì²˜ë¥¼ í¬í•¨í•˜ì„¸ìš”. ì˜ˆ: [1]
5. ë‹µë³€ í•˜ë‹¨ì— ì¶œì²˜ë¥¼ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ì„¸ìš”.
6. ë¶ˆí•„ìš”í•œ ê°œì¸ ì •ë³´ ë…¸ì¶œ, ì „ë¬¸ê°€ì˜ ì‹¤ëª… ì–¸ê¸‰ ë“±ì€ í”¼í•´ì£¼ì„¸ìš”."""


def generate_answer(state: InterviewState):
    analyst = state["analyst"]
    messages = state["messages"]
    context = state["context"]

    if not context:
        context = ["ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]

    system_message = answer_instructions.format(
        goals=analyst.description,
        context="\n\n".join(context)
    )

    try:
        response = llm.invoke([
            SystemMessage(content=system_message),
            HumanMessage(content="\nì»¨í…ìŠ¤íŠ¸:\n" + "\n".join(context) +
                         "\n\nì§ˆë¬¸:\n" + messages[-1].content)
        ])

        answer = AIMessage(content=response.content, name="expert")
        return {"messages": [answer]}

    except Exception as e:
        print(f"Error generating answer: {e}")
        return {"messages": [AIMessage(content="ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", name="expert")]}


def save_interview(state: InterviewState):
    messages = state["messages"]
    interview = get_buffer_string(messages)
    return {"interview": interview}


def route_messages(state: InterviewState, name: str = "expert"):
    messages = state["messages"]
    max_num_turns = state.get("max_num_turns", 2)

    num_responses = len(
        [m for m in messages if isinstance(m, AIMessage) and m.name == name]
    )

    # ì „ë¬¸ê°€ê°€ ìµœëŒ€ í„´ ìˆ˜ ì´ìƒ ë‹µë³€í•œ ê²½ìš° ì¢…ë£Œ
    if num_responses >= max_num_turns:
        return "save_interview"

    # ë§ˆì§€ë§‰ ì§ˆì˜ì— "ë„ì›€ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤" ë“±ì˜ í‘œí˜„ì´ ìˆìœ¼ë©´ ì¢…ë£Œ
    last_question = messages[-2] if len(messages) >= 2 else None
    if last_question and "ë„ì›€ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤" in last_question.content:
        return "save_interview"

    return "ask_question"


section_writer_instructions = """ë‹¹ì‹ ì€ ì „ë¬¸ ê¸°ìˆ  ì‘ê°€ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì†ŒìŠ¤ ë¬¸ì„œ ì„¸íŠ¸ë¥¼ ì² ì €íˆ ë¶„ì„í•˜ì—¬ ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ë³´ê³ ì„œ ì„¹ì…˜ì„ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
- ë³´ê³ ì„œì— ëŒ€í•œ êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ ì–‘ì‹ í™œìš©
- ìƒì„¸í•˜ê³  ë…¼ë¦¬ì ì¸ ì „ê°œ
- ê°€ëŠ¥í•œ í•œ í’ë¶€í•œ ì˜ˆì‹œì™€ ê·¼ê±°ë¥¼ ì œì‹œ
- ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ ì–´ì¡° ìœ ì§€
- ìµœì†Œ 800ë‹¨ì–´ ì´ìƒ

(í•„ìš” ì‹œ ìˆ˜ì • ê°€ëŠ¥)"""


def write_section(state: InterviewState):
    context = state["context"]
    analyst = state["analyst"]

    system_message = section_writer_instructions  # í•„ìš”í•˜ë‹¤ë©´ formatìœ¼ë¡œ analyst ì •ë³´ ì‚¬ìš©
    section = llm.invoke(
        [SystemMessage(content=system_message)]
        + [HumanMessage(content=f"ì´ ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¹ì…˜ì„ ì‘ì„±í•˜ì„¸ìš”: {context}")]
    )

    return {"sections": [section.content]}


##############################
# 7) ì¸í„°ë·°ìš© ê·¸ë˜í”„ ì •ì˜
##############################
stock_interview_builder = StateGraph(InterviewState)
stock_interview_builder.add_node("ask_question", generate_question)
stock_interview_builder.add_node("search_web", search_web)
stock_interview_builder.add_node("search_youtube", search_youtube)
stock_interview_builder.add_node("answer_question", generate_answer)
stock_interview_builder.add_node("save_interview", save_interview)
stock_interview_builder.add_node("write_section", write_section)

stock_interview_builder.add_edge(START, "ask_question")
stock_interview_builder.add_edge("ask_question", "search_web")
stock_interview_builder.add_edge("ask_question", "search_youtube")
stock_interview_builder.add_edge("search_web", "answer_question")
stock_interview_builder.add_edge("search_youtube", "answer_question")
stock_interview_builder.add_conditional_edges(
    "answer_question", route_messages, ["ask_question", "save_interview"]
)
stock_interview_builder.add_edge("save_interview", "write_section")
stock_interview_builder.add_edge("write_section", END)

memory2 = MemorySaver()
stock_interview_graph = stock_interview_builder.compile(checkpointer=memory2).with_config(
    run_name="Conduct Interviews"
)

# í•„ìš” ì‹œ ì‹œê°í™”
# visualize_graph(stock_interview_graph)


##############################
# 8) í†µí•©ëœ ë¦¬ì„œì¹˜/ë³´ê³ ì„œ ê·¸ë˜í”„
##############################
class ResearchGraphState(TypedDict):
    topic: str
    max_analysts: int
    human_analyst_feedback: str
    analysts: List[KDBanalyst]
    sections: Annotated[list, operator.add]
    introduction: str
    content: str
    conclusion: str
    final_report: str


def initiate_all_interviews(state: ResearchGraphState):
    human_analyst_feedback = state.get("human_analyst_feedback")
    if human_analyst_feedback:
        return "create_analysts"
    else:
        topic = state["topic"]
        return [
            Send(
                "conduct_interview",
                {
                    "analyst": analyst,
                    "messages": [
                        HumanMessage(
                            content=f"So you said you were writing an article on {topic}?"
                        )
                    ],
                },
            )
            for analyst in state["analysts"]
        ]


report_writer_instructions = """ë‹¹ì‹ ì€ ë‹¤ìŒ ì£¼ì œì— ëŒ€í•œ íˆ¬ì ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤:

{topic}

ì—¬ëŸ¬ ë¶„ì„ê°€ë“¤ì´ ê°ê° ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤:

1. í•´ë‹¹ ë¶„ì•¼ ì „ë¬¸ê°€ì™€ì˜ ì¸í„°ë·° ì§„í–‰
2. ë¶„ì„ ë‚´ìš©ì„ ë©”ëª¨ë¡œ ì •ë¦¬

ë‹¹ì‹ ì˜ ì„ë¬´:

1. ê° ë¶„ì„ê°€ë“¤ì˜ ë©”ëª¨ë¥¼ ê²€í† í•©ë‹ˆë‹¤.
2. ê° ë©”ëª¨ì˜ í•µì‹¬ ë‚´ìš©ì„ ë©´ë°€íˆ ë¶„ì„í•©ë‹ˆë‹¤.
3. ëª¨ë“  ë©”ëª¨ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ í†µí•©í•˜ì—¬ ì¢…í•©ì ì¸ ìš”ì•½ì„ ì‘ì„±í•©ë‹ˆë‹¤.
4. ê° ë©”ëª¨ì˜ ì£¼ìš” í¬ì¸íŠ¸ë¥¼ ì•„ë˜ ì„¹ì…˜ì— ë§ê²Œ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
5. ëª¨ë“  í•„ìˆ˜ ì„¹ì…˜ì„ `### ì„¹ì…˜ëª…` í˜•ì‹ì˜ í—¤ë”ë¡œ í¬í•¨ì‹œí‚µë‹ˆë‹¤.
6. ê° ì„¹ì…˜ë‹¹ ì•½ 250ì ë‚´ì™¸ë¡œ ì‹¬ë„ìˆëŠ” ì„¤ëª…ê³¼ ê·¼ê±°ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

**ë³´ê³ ì„œ ì„¹ì…˜ êµ¬ì„±:**

- **ì‹œì¥ í™˜ê²½**
- **ì‚°ì—… ë¶„ì„**
- **ê¸°ì—… ë¶„ì„**
- **íˆ¬ì í¬ì¸íŠ¸**
- **ë¦¬ìŠ¤í¬ ìš”ì¸**
- **ì‹¤ì  ì „ë§**
- **íˆ¬ìì˜ê²¬**

ë³´ê³ ì„œ í˜•ì‹:
1. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì‚¬ìš©
2. ì„œë‘ ì—†ì´ ë°”ë¡œ ë³¸ë¬¸ ì‹œì‘
3. ì†Œì œëª© ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
4. ë³´ê³ ì„œëŠ” ## íˆ¬ì ë¶„ì„ í—¤ë”ë¡œ ì‹œì‘
5. ë¶„ì„ê°€ ì´ë¦„ ì–¸ê¸‰í•˜ì§€ ì•ŠìŒ
6. ë©”ëª¨ì˜ ì¸ìš© ì¶œì²˜ëŠ” [1], [2] ë“±ìœ¼ë¡œ í‘œì‹œ
7. ë§ˆì§€ë§‰ì— ## ì°¸ê³ ìë£Œ ì„¹ì…˜ì— ì¶œì²˜ ëª©ë¡ ì •ë¦¬
8. ì¶œì²˜ëŠ” ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ê³  ì¤‘ë³µ ì œê±°
"""


def write_report(state: ResearchGraphState):
    sections = state["sections"]
    topic = state["topic"]
    formatted_str_sections = "\n\n".join([f"{section}" for section in sections])

    system_message = report_writer_instructions.format(
        topic=topic, context=formatted_str_sections
    )
    report = llm.invoke(
        [SystemMessage(content=system_message)]
        + [HumanMessage(content=f"ì´ ë©”ëª¨ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.")]
    )
    return {"content": report.content}


intro_conclusion_instructions = """ë‹¹ì‹ ì€ {topic}ì— ëŒ€í•œ íˆ¬ì ë³´ê³ ì„œë¥¼ ë§ˆë¬´ë¦¬í•˜ëŠ” ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

ë³´ê³ ì„œì˜ ëª¨ë“  ì„¹ì…˜ì´ ì£¼ì–´ì§ˆ ê²ƒì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ê°„ê²°í•˜ê³  ì„¤ë“ë ¥ ìˆëŠ” ì„œë¡  ë˜ëŠ” ê²°ë¡ ì„ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë¶ˆí•„ìš”í•œ ì„œë‘ëŠ” ìƒëµí•©ë‹ˆë‹¤.

ì•½ 200ì ë‚´ì™¸ë¡œ, ì„œë¡ ì˜ ê²½ìš° ë³´ê³ ì„œì˜ ëª¨ë“  ì„¹ì…˜ì„ ë¯¸ë¦¬ë³´ê¸°í•˜ê³ , ê²°ë¡ ì˜ ê²½ìš° í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•©ë‹ˆë‹¤.

ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì„œë¡ ì˜ ê²½ìš° ë§¤ë ¥ì ì¸ ì œëª©ì„ ë§Œë“¤ê³  # í—¤ë”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì„œë¡ ì€ ## ê°œìš” í—¤ë”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ê²°ë¡ ì€ ## ê²°ë¡  í—¤ë”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì°¸ê³ í•  ì„¹ì…˜ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {formatted_str_sections}"""


def write_introduction(state: ResearchGraphState):
    sections = state["sections"]
    topic = state["topic"]
    formatted_str_sections = "\n\n".join([f"{section}" for section in sections])

    instructions = intro_conclusion_instructions.format(
        topic=topic, formatted_str_sections=formatted_str_sections
    )
    intro = llm.invoke(
        [SystemMessage(content=instructions)]
        + [HumanMessage(content=f"ë³´ê³ ì„œì˜ ì„œë¡ ì„ ì‘ì„±í•´ì£¼ì„¸ìš”")]
    )
    return {"introduction": intro.content}


def write_conclusion(state: ResearchGraphState):
    sections = state["sections"]
    topic = state["topic"]
    formatted_str_sections = "\n\n".join([f"{section}" for section in sections])

    instructions = intro_conclusion_instructions.format(
        topic=topic, formatted_str_sections=formatted_str_sections
    )
    conclusion = llm.invoke(
        [SystemMessage(content=instructions)]
        + [HumanMessage(content=f"ë³´ê³ ì„œì˜ ê²°ë¡ ì„ ì‘ì„±í•´ì£¼ì„¸ìš”")]
    )
    return {"conclusion": conclusion.content}


def finalize_report(state: ResearchGraphState):
    content = state["content"]
    # "## íˆ¬ì ë¶„ì„" ì œê±°(í•„ìš” ì‹œ)
    if content.startswith("## íˆ¬ì ë¶„ì„"):
        content = content.strip("## íˆ¬ì ë¶„ì„")

    if "## ì°¸ê³ ìë£Œ" in content:
        try:
            content, sources = content.split("\n## ì°¸ê³ ìë£Œ\n", maxsplit=1)
        except:
            sources = None
    else:
        sources = None

    final_report = (
        state["introduction"]
        + "\n\n---\n\n## í•µì‹¬ íˆ¬ìí¬ì¸íŠ¸\n\n"
        + content
        + "\n\n---\n\n"
        + state["conclusion"]
    )
    if sources is not None:
        final_report += "\n\n## ì°¸ê³ ìë£Œ\n" + sources

    return {"final_report": final_report}


##############################
# 9) ìµœì¢… ë¦¬ì„œì¹˜ ê·¸ë˜í”„ êµ¬ì„±
##############################
research_builder = StateGraph(ResearchGraphState)

research_builder.add_node("create_analysts", create_analysts)
research_builder.add_node("human_feedback", human_feedback)
research_builder.add_node("conduct_interview", stock_interview_graph)
research_builder.add_node("write_report", write_report)
research_builder.add_node("write_introduction", write_introduction)
research_builder.add_node("write_conclusion", write_conclusion)
research_builder.add_node("finalize_report", finalize_report)

research_builder.add_edge(START, "create_analysts")
research_builder.add_edge("create_analysts", "human_feedback")
research_builder.add_conditional_edges(
    "human_feedback", initiate_all_interviews, ["create_analysts", "conduct_interview"]
)
research_builder.add_edge("conduct_interview", "write_report")
research_builder.add_edge("conduct_interview", "write_introduction")
research_builder.add_edge("conduct_interview", "write_conclusion")
research_builder.add_edge(["write_conclusion", "write_report", "write_introduction"], "finalize_report")
research_builder.add_edge("finalize_report", END)

memory3 = MemorySaver()
research_graph = research_builder.compile(interrupt_before=["human_feedback"], checkpointer=memory3)