{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# User Agent 설정\n",
    "ua = UserAgent()\n",
    "headers = {\n",
    "    \"User-Agent\": ua.random\n",
    "}\n",
    "\n",
    "# 기준 날짜 설정 (2025년 1월 1일)\n",
    "cutoff_date = datetime.strptime(\"20250103\", \"%Y%m%d\")\n",
    "total_articles_processed = 0\n",
    "\n",
    "# 회사 정보 직접 설정\n",
    "companies = {\n",
    "    '삼성전자': '005930',\n",
    "    'SK하이닉스': '000660',\n",
    "    'LG에너지솔루션': '373220', \n",
    "    '삼성바이오로직스': '207940',\n",
    "    '삼성SDI': '006400',\n",
    "    'LG화학': '051910',\n",
    "    '현대자동차': '005380',\n",
    "    '포스코홀딩스': '005490',\n",
    "    '삼성전자우': '005935',\n",
    "    '기아': '000270',\n",
    "    'NAVER': '035420',\n",
    "    'LG전자': '066570',\n",
    "    '현대모비스': '012330',\n",
    "    'SK이노베이션': '096770',\n",
    "    'SK': '034730',\n",
    "    'LG': '003550',\n",
    "    'KB금융': '105560',\n",
    "    '신한지주': '055550',\n",
    "    '카카오': '035720',\n",
    "    '삼성물산': '028260',\n",
    "    'KT&G': '033780',\n",
    "    '하나금융지주': '086790',\n",
    "    '포스코퓨처엠': '003670',\n",
    "    '삼성생명': '032830',\n",
    "    'SK텔레콤': '017670',\n",
    "    '카카오뱅크': '323410',\n",
    "    '현대중공업': '329180',\n",
    "    'KT': '030200',\n",
    "    '기업은행': '024110',\n",
    "    '삼성화재': '000810',\n",
    "    'S-Oil': '010950',\n",
    "    '두산에너빌리티': '034020',\n",
    "    '고려아연': '010130',\n",
    "    '우리금융지주': '316140',\n",
    "    'HMM': '011200',\n",
    "    '한화에어로스페이스': '012450',\n",
    "    '한국전력': '015760',\n",
    "    '삼성전기': '009150',\n",
    "    'HD현대': '267250',\n",
    "    '한온시스템': '018880',\n",
    "    '한화솔루션': '009830',\n",
    "    '현대글로비스': '086280',\n",
    "    'SK스퀘어': '402340',\n",
    "    '셀트리온': '068270',\n",
    "    '한국조선해양': '009540',\n",
    "    '두산밥캣': '241560',\n",
    "    'HD한국조선해양': '042660',\n",
    "    '맥쿼리인프라': '088980',\n",
    "    '삼성증권': '016360',\n",
    "    '한미사이언스': '008930'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # PostgreSQL 연결\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"stockhelper\",\n",
    "        user=\"kdb\", \n",
    "        password=\"1234\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # news 테이블이 없다면 생성\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS news (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            company VARCHAR(100),\n",
    "            stock_code VARCHAR(20),\n",
    "            timestamp TIMESTAMP,\n",
    "            title TEXT,\n",
    "            press VARCHAR(100),\n",
    "            summary TEXT,\n",
    "            content TEXT,\n",
    "            url TEXT UNIQUE,\n",
    "            origin_url TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    for company, code in tqdm(companies.items(), desc=\"Companies\", total=len(companies)):\n",
    "        try:\n",
    "            # 첫 페이지만 조회 (최신 뉴스 20개)\n",
    "            url = f\"https://m.stock.naver.com/api/news/stock/{code}?pageSize=20&page=1\"\n",
    "            \n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if not isinstance(data, list):\n",
    "                data = [data] if data else []\n",
    "            \n",
    "            if len(data) == 0:\n",
    "                print(f\"No news found for {company}\")\n",
    "                continue\n",
    "            \n",
    "            # 가장 최신 뉴스 찾기\n",
    "            latest_article = None\n",
    "            \n",
    "            for article in data:\n",
    "                try:\n",
    "                    if 'items' not in article or not article['items']:\n",
    "                        continue\n",
    "                        \n",
    "                    article_data = article['items'][0]\n",
    "                    \n",
    "                    # 날짜 확인\n",
    "                    article_date = datetime.strptime(article_data['datetime'], \"%Y%m%d%H%M\")\n",
    "                    if article_date > cutoff_date:\n",
    "                        continue\n",
    "                    \n",
    "                    # 첫 번째 유효한 기사를 찾으면 저장하고 종료\n",
    "                    news_url = f\"https://n.news.naver.com/mnews/article/{article_data['officeId']}/{article_data['articleId']}\"\n",
    "                    \n",
    "                    article_response = requests.get(news_url, headers=headers)\n",
    "                    article_response.raise_for_status()\n",
    "                    soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "                    \n",
    "                    title = soup.select_one('#title_area > span')\n",
    "                    if not title:\n",
    "                        continue\n",
    "                    title = title.text.strip()\n",
    "                    \n",
    "                    press = article_data['officeName']\n",
    "                    \n",
    "                    content_element = soup.select_one('#dic_area')\n",
    "                    content = content_element.get_text(strip=True) if content_element else ''\n",
    "                    summary = article_data.get('body', '')\n",
    "                    \n",
    "                    origin_element = soup.select_one('#ct > div.media_end_head.go_trans > div.media_end_head_info.nv_notrans > div.media_end_head_info_datestamp > a')\n",
    "                    origin_url = origin_element['href'] if origin_element else news_url\n",
    "\n",
    "                    # 문서 저장\n",
    "                    insert_query = \"\"\"\n",
    "                        INSERT INTO news (company, stock_code, timestamp, title, press, \n",
    "                                        summary, content, url, origin_url, created_at)\n",
    "                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                        ON CONFLICT (url) DO UPDATE SET\n",
    "                            company = EXCLUDED.company,\n",
    "                            stock_code = EXCLUDED.stock_code,\n",
    "                            timestamp = EXCLUDED.timestamp,\n",
    "                            title = EXCLUDED.title,\n",
    "                            press = EXCLUDED.press,\n",
    "                            summary = EXCLUDED.summary,\n",
    "                            content = EXCLUDED.content,\n",
    "                            origin_url = EXCLUDED.origin_url,\n",
    "                            created_at = CURRENT_TIMESTAMP\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    cur.execute(insert_query, (\n",
    "                        company,\n",
    "                        code,\n",
    "                        article_date,\n",
    "                        title,\n",
    "                        press,\n",
    "                        summary,\n",
    "                        content,\n",
    "                        news_url,\n",
    "                        origin_url,\n",
    "                        datetime.now()\n",
    "                    ))\n",
    "                    \n",
    "                    conn.commit()\n",
    "                    total_articles_processed += 1\n",
    "                    print(f\"Saved news for {company}: {title[:50]}...\")\n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article for {company}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "            time.sleep(random.uniform(0.2, 0.4))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing company {company}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Major error occurred: {str(e)}\")\n",
    "finally:\n",
    "    print(\"\\nCrawling completed!\")\n",
    "    print(f\"Total articles processed: {total_articles_processed}\")\n",
    "    if 'cur' in locals():\n",
    "        cur.close()\n",
    "    if 'conn' in locals():\n",
    "        conn.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
